{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertoarturomc/ProgramacionConcurrente/blob/main/21_Introduccion_a_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UQAJS17EEaI"
      },
      "source": [
        "### Programación Concurrente\n",
        "## 21. Introducción a Spark\n",
        "\n",
        "Spark es la herramienta, dentro del ecosistema de Hadoop, que permite trabajar con Cómputo Paralelo y Distribuido.\n",
        "\n",
        "Para ello, un ambiente de Spark necesita contar con los siguientes elementos.\n",
        "\n",
        "\n",
        "![Hilos](https://d39kqat1wpn1o5.cloudfront.net/app/uploads/2019/10/spark-architecture-1024x542.png)\n",
        "\n",
        "- El Master Node contiene el Drive: distribuye y asigna tareas a los Workers.\n",
        "- Los Worker Nodes ejecutan las tareas.\n",
        "\n",
        "(Nota: estos elementos son Procesos al fin y al cabo, entonces pueden ser ejecutados en una sola computadora, si trabajamos de manera local).\n",
        "\n",
        "- El Cluster Manager administra los recursos, para que las tareas puedan ser completados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Particiones\n",
        "\n",
        "Para poder paralelizar tareas, nuestros DataFrames de Spark se guardan con particiones: es decir, un grupo de hileras serán procesados un Worker nodo, otras por otro nodo, etc. Las particiones normalmente se crean en automático.\n",
        "\n",
        "### Lazy Evaluation\n",
        "\n",
        "Además, Spark no ejecuta las tareas de inmediato. En realidad, se espera a tener un grupo de tareas y se espera hasta que estas realmente tengan que ser ejecutadas (por ejemplo, al mandar a guardar un output), para empezar el procesamiento.\n",
        "\n",
        "Esto lo hace porque se espera a tener un set de instrucciones y así poder optimizarlas."
      ],
      "metadata": {
        "id": "YbOTqZ5KSrvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iniciar un programa con Spark\n",
        "\n",
        "Para empezar a trabajar con Spark, necesitamos forzosamente crear una SparkSession. Además, si no lo hemos hecho ya, es necesario installar e importar la librería `pyspark`.\n",
        "\n"
      ],
      "metadata": {
        "id": "83i9fVOYeIFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "q_D_a4emeeSP",
        "outputId": "f7c2a810-5404-4dd9-8ae1-6b990e78bf57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.0\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317123 sha256=2e507e3500097036fe8f1b1f89385455feb424fbee0a6f4c11f3f46f74aeb8cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.3\n",
            "    Uninstalling pyspark-3.5.3:\n",
            "      Successfully uninstalled pyspark-3.5.3\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyspark"
                ]
              },
              "id": "55197621907f43a6a66795007b86adac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "tUQv3yP_ekQQ",
        "outputId": "fabdff2c-4fed-411b-8926-74a5bd9577db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x783d340a6f80>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5f3ad6feca0a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a trabajar con datos del Covid-19 en México. Revísalos y descárgalos de la siguiente página web:\n",
        "\n",
        "https://www.kaggle.com/datasets/tavoglc/covid19-data-from-mexico?resource=download"
      ],
      "metadata": {
        "id": "W98URV76_oEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('covidmex.csv',header=True)\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "ZCo9idB7-pKp",
        "outputId": "c9fa9203-3e73-443f-8aed-35089d956121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-------------+-------------+--------------+------+-----+------------------+-------------------+------------------+--------------------+---------+------------------+\n",
            "|ID_REGISTRO|ENTIDAD_RES|MUNICIPIO_RES|FECHA_INGRESO|FECHA_SINTOMAS|covidt|delta|               lat|               long|               alt|                 qry|dayofyear|       lengthofday|\n",
            "+-----------+-----------+-------------+-------------+--------------+------+-----+------------------+-------------------+------------------+--------------------+---------+------------------+\n",
            "|     z526b3|          9|           12|   2020-12-21|    2020-12-18|     1|    3| 19.20155345588235| -99.20180252450989|3008.9460784313724|Cve_Ent==9 & Cve_...|      356|11.768371962176587|\n",
            "|     z3d1e2|          9|            5|   2020-04-22|    2020-04-20|     1|    2|         19.482945|         -99.113471|            2229.0|Cve_Ent==9 & Cve_...|      113|13.441805501598786|\n",
            "|     z21f6f|          7|            9|   2020-04-27|    2020-04-24|     1|    3| 16.21599257359306| -93.93758461688306| 83.17316017316017|Cve_Ent==7 & Cve_...|      118|13.399116765412083|\n",
            "|     zz9040|         19|           45|   2020-09-06|    2020-09-03|     1|    3| 26.06980561012655|-100.27776220506335| 486.4253164556962|Cve_Ent==19 & Cve...|      250|13.288478608721755|\n",
            "|     z21c58|         19|           26|   2020-07-10|    2020-07-06|     1|    4|25.679631916666665|-100.16954308333334|            551.75|Cve_Ent==19 & Cve...|      192|14.507914985048014|\n",
            "+-----------+-----------+-------------+-------------+--------------+------+-----+------------------+-------------------+------------------+--------------------+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análisis exploratorio usando SparkDataFrames\n",
        "\n",
        "Manipular datos y hacer cálculos a veces es similar, pero normalmente es distinto a como lo haríamos en Pandas. De hecho, ya viste una diferencia: el uso del método `.show(n)`\n",
        "\n",
        "Otras diferencias son:"
      ],
      "metadata": {
        "id": "bSK_lICj_yfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conocer la dimension de mi DataFrame\n",
        "\n",
        "print((df.count(), len(df.columns)))"
      ],
      "metadata": {
        "id": "cWqo7wA6-oV8",
        "outputId": "edb88461-d76e-41f1-944d-9f85b6971f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(903871, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ¿Qué tipos de datos tengo?\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "kzY1ooW8CSwK",
        "outputId": "31c4c70a-f1f4-42c9-cbb2-32cc98f87ece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID_REGISTRO: string (nullable = true)\n",
            " |-- ENTIDAD_RES: string (nullable = true)\n",
            " |-- MUNICIPIO_RES: string (nullable = true)\n",
            " |-- FECHA_INGRESO: string (nullable = true)\n",
            " |-- FECHA_SINTOMAS: string (nullable = true)\n",
            " |-- covidt: string (nullable = true)\n",
            " |-- delta: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- long: string (nullable = true)\n",
            " |-- alt: string (nullable = true)\n",
            " |-- qry: string (nullable = true)\n",
            " |-- dayofyear: string (nullable = true)\n",
            " |-- lengthofday: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tarea\n",
        "\n",
        "Aplica los cambios siguientes:\n",
        "1. Modifica tu DataFrame: agrega una columna nueva llamada \"PAIS\", en donde todas las hileras tendrán el valor \"Mexico\".\n",
        "\n",
        "2. Elimina la columna *lengthofday* de tu DataFrame.\n",
        "\n",
        "3. Crea un nuevo DataFrame, llamado `ubicacion`, que tendrá solamente los valores de *lat*, *lon*, *ENTIDAD_RES* y *PAIS* de `df`.\n",
        "\n",
        "4. Filtra la columna *ENTIDAD_RES* dentro de `ubicacion` para quedarte con solo los valores = 9 (Ciudad de México). Guarda el resultado en un DataFrame llamado `cdmx`\n",
        "\n",
        "5. Selecciona los primeros 5 valores de `cdmx` y guárdalos como un DataFrame de Pandas, con el nombre *df_pandas*.\n",
        "\n",
        "####Tip\n",
        "Métodos y Funciones que podrían servirte:\n",
        "\n",
        "- .select()\n",
        "- .drop()\n",
        "- .withcolumn()\n",
        "- lit()\n",
        "- .filter()\n",
        "- .limit()\n",
        "- toPandas()\n",
        "\n"
      ],
      "metadata": {
        "id": "qkRh-aKEAvnT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qFb9CAX7BdP1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}