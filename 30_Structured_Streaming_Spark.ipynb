{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertoarturomc/ProgramacionConcurrente/blob/main/30_Structured_Streaming_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UQAJS17EEaI"
      },
      "source": [
        "### Programación Concurrente\n",
        "## 30. Structured Streaming con Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Streaming con una API Pública (USGS – Terremotos)\n",
        "\n",
        "### ¿Qué es Structured Streaming?\n",
        "\n",
        "**Apache Spark Structured Streaming** es un motor de procesamiento de datos en tiempo real que funciona como si estuviéramos ejecutando consultas de Spark *continuamente*.\n",
        "\n",
        "Su idea principal es escribir código de análisis de datos como si fuera batch, pero Spark lo ejecuta continuamente cada vez que llegan nuevos datos.\n",
        "\n",
        "### Conceptos clave:\n",
        "\n",
        "**Flujo de datos (stream)**  \n",
        "Es una fuente que *produce datos gradualmente*\n",
        "\n",
        "API, sensores, Kafka, archivos que llegan, logs, etc.\n",
        "\n",
        "**Micro-batches**  \n",
        "Spark no procesa cada evento uno por uno, sino pequeños “mini batchs” que agrupan los nuevos datos que llegaron desde la última ejecución.\n",
        "\n",
        "**Fuentes de streaming (streaming sources)**  \n",
        "Ejemplos: Kafka, sockets, carpetas donde llegan archivos, APIs, streams internos de Spark.\n",
        "\n",
        "**Sinks (destinos)**  \n",
        "Dónde escribimos los resultados: consola, archivos, Kafka, bases de datos, etc.\n",
        "\n",
        "**Event time vs Processing time**\n",
        "- *Processing time:* cuándo Spark procesa los datos.  \n",
        "- *Event time:* cuándo *ocurrieron* realmente los eventos.  \n",
        "Structured Streaming puede agrupar por ventanas usando *event time*.\n",
        "\n",
        "**Watermark**  \n",
        "Un mecanismo para manejar datos que llegan tarde."
      ],
      "metadata": {
        "id": "aYxzgK4TmFLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¿Qué haremos en este notebook?\n",
        "\n",
        "1. Llamaremos repetidamente a una **API pública real** (USGS Earthquakes).  \n",
        "2. Cada llamada guardará un archivo NDJSON con varios sismos.  \n",
        "3. Spark Structured Streaming vigilará una carpeta e irá leyendo cada archivo conforme aparece.  \n",
        "4. Procesaremos los datos en *tiempo casi real*:  \n",
        "- Contando número de sismos cada ventana de 10 minutos  \n",
        "- Calculando magnitud media y máxima  \n",
        "5. Veremos los resultados aparecer en la consola conforme llegan archivos nuevos.\n",
        "\n",
        "Es un ejemplo **muy realista** de cómo se integran:\n",
        "- APIs → ingestión (landing layer)  \n",
        "- Procesamiento streaming → agregaciones con ventanas  \n",
        "- Tolerancia a retrasos → watermark  \n",
        "- Resultados incrementalmente actualizados  "
      ],
      "metadata": {
        "id": "Tx8rxUOgm07Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, LongType, DoubleType\n",
        ")\n",
        "from pyspark.sql.functions import col, from_unixtime, to_timestamp\n",
        "from pyspark.sql.functions import window, count, avg, max as spark_max\n"
      ],
      "metadata": {
        "id": "dfe5Uhzan6ZV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (SparkSession.builder\n",
        "         .appName(\"QuakeStructuredStream\")\n",
        "         .master(\"local[*]\")\n",
        "         .getOrCreate())\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "z5HSZomRoZib",
        "outputId": "11468d6d-f1ce-4b80-db8b-013ee63674ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7900f74d6f30>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e7c71137a963:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>QuakeStructuredStream</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Crear carpetas para simular el streaming\n",
        "- `stream_data/` → Aquí Spark estará escuchando por nuevos archivos NDJSON.\n",
        "\n",
        "Nuestro proceso de Python (más adelante) irá guardando nuevos archivos ahí.\n"
      ],
      "metadata": {
        "id": "l995l_KanLFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/earthquakes\"\n",
        "stream_dir = os.path.join(base_dir, \"stream_data\")\n",
        "\n",
        "os.makedirs(stream_dir, exist_ok=True)\n",
        "\n",
        "print(\"Carpeta para streaming:\", stream_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbReXQVwnUPq",
        "outputId": "dd9e2d94-445d-4f16-c07a-8654c609e98e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carpeta para streaming: /content/earthquakes/stream_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Proceso de ingesta desde la API pública\n",
        "\n",
        "Usaremos la API de USGS:\n",
        "\n",
        "https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson\n",
        "\n",
        "Esta API devuelve **todos los sismos de la última hora** (actualizados continuamente).\n",
        "\n",
        "Nuestra función:\n",
        "- Llama a la API varias veces.\n",
        "- Crea un archivo .ndjson por cada llamada.\n",
        "- Spark verá estos archivos como \"nuevos eventos\".\n"
      ],
      "metadata": {
        "id": "jDabhsc3neMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_and_save_earthquakes(output_dir, iterations=3, sleep_seconds=15):\n",
        "    \"\"\"\n",
        "    Llama a la API de USGS varias veces y guarda archivos NDJSON en output_dir.\n",
        "    \"\"\"\n",
        "    url = \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson\"\n",
        "\n",
        "    for i in range(iterations):\n",
        "        print(f\"[{datetime.utcnow().isoformat()}] Llamando a la API ({i+1}/{iterations})...\")\n",
        "        resp = requests.get(url)\n",
        "        data = resp.json()\n",
        "\n",
        "        features = data.get(\"features\", [])\n",
        "        print(f\"  → {len(features)} eventos recibidos\")\n",
        "\n",
        "        ts = int(time.time())\n",
        "        out_path = os.path.join(output_dir, f\"earthquakes_{ts}.ndjson\")\n",
        "\n",
        "        with open(out_path, \"w\") as f:\n",
        "            for feat in features:\n",
        "                props = feat.get(\"properties\", {})\n",
        "                geom = feat.get(\"geometry\", {})\n",
        "\n",
        "                record = {\n",
        "                    \"time\": props.get(\"time\"),      # epoch ms\n",
        "                    \"place\": props.get(\"place\"),\n",
        "                    \"mag\": props.get(\"mag\"),\n",
        "                    \"alert\": props.get(\"alert\"),\n",
        "                    \"tsunami\": props.get(\"tsunami\"),\n",
        "                    \"longitude\": geom.get(\"coordinates\", [None])[0],\n",
        "                    \"latitude\":  geom.get(\"coordinates\", [None])[1],\n",
        "                    \"depth\":     geom.get(\"coordinates\", [None])[2],\n",
        "                }\n",
        "                f.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "        print(f\"  → Archivo escrito: {out_path}\\n\")\n",
        "        if i < iterations - 1:\n",
        "            time.sleep(sleep_seconds)\n"
      ],
      "metadata": {
        "id": "VQ30WkILndn9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Definir el stream de lectura en Spark\n",
        "\n",
        "Ahora que ya sabemos que aparecerán archivos .ndjson en `stream_data/`,\n",
        "configuramos Spark para:\n",
        "\n",
        "- Leer archivos JSON de esa carpeta.  \n",
        "- Convertir el campo `time` (ms desde epoch) a un timestamp de Spark.  \n",
        "- Preparar los datos para agregaciones de ventanas.\n"
      ],
      "metadata": {
        "id": "DIUNaX1Bn_u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"time\", LongType(), True),\n",
        "    StructField(\"place\", StringType(), True),\n",
        "    StructField(\"mag\", DoubleType(), True),\n",
        "    StructField(\"alert\", StringType(), True),\n",
        "    StructField(\"tsunami\", DoubleType(), True),\n",
        "    StructField(\"longitude\", DoubleType(), True),\n",
        "    StructField(\"latitude\", DoubleType(), True),\n",
        "    StructField(\"depth\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "raw_stream_df = (\n",
        "    spark.readStream\n",
        "         .format(\"json\")\n",
        "         .schema(schema)\n",
        "         .load(stream_dir)\n",
        ")\n",
        "\n",
        "stream_df = raw_stream_df.withColumn(\n",
        "    \"event_time\",\n",
        "    to_timestamp(from_unixtime(col(\"time\") / 1000))\n",
        ")\n",
        "\n",
        "stream_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmYKifQ1n2C7",
        "outputId": "94c07078-217a-4974-bb27-aae804a98143"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- time: long (nullable = true)\n",
            " |-- place: string (nullable = true)\n",
            " |-- mag: double (nullable = true)\n",
            " |-- alert: string (nullable = true)\n",
            " |-- tsunami: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- depth: double (nullable = true)\n",
            " |-- event_time: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Agregaciones en tiempo real\n",
        "\n",
        "Queremos responder preguntas como:\n",
        "\n",
        "**“¿Cuántos terremotos hubo cada 10 minutos y cuál fue su magnitud promedio y máxima?”**\n",
        "\n",
        "Para eso:\n",
        "- Usamos `groupBy(window(event_time, \"10 minutes\"))`  \n",
        "- Añadimos un `watermark` para tolerar datos tardíos  \n",
        "- Calculamos `quake_count`, `avg_mag`, `max_mag`\n"
      ],
      "metadata": {
        "id": "4LGWgz9Mooby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_df = (\n",
        "    stream_df\n",
        "    .withWatermark(\"event_time\", \"30 minutes\")\n",
        "    .groupBy(window(col(\"event_time\"), \"10 minutes\"))\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"quake_count\"),\n",
        "        avg(\"mag\").alias(\"avg_mag\"),\n",
        "        spark_max(\"mag\").alias(\"max_mag\")\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "agg_df.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tacLFq0oN77",
        "outputId": "475ecc15-13bb-4efb-ca66-075feb842f0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- window: struct (nullable = false)\n",
            " |    |-- start: timestamp (nullable = true)\n",
            " |    |-- end: timestamp (nullable = true)\n",
            " |-- quake_count: long (nullable = false)\n",
            " |-- avg_mag: double (nullable = true)\n",
            " |-- max_mag: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Iniciar el streaming query\n",
        "\n",
        "Esto iniciará un *job continuo* que:\n",
        "- Observa la carpeta `stream_data/`\n",
        "- Espera a que aparezcan archivos nuevos\n",
        "- Procesa datos cuando llegan\n",
        "- Actualiza las agregaciones\n",
        "\n",
        "**Nota: Deja esta celda corriendo.**  \n",
        "En otra celda correremos la función que llama a la API.\n"
      ],
      "metadata": {
        "id": "EHWvvlUYo6Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.streaming.noDataMicroBatch.enabled\", \"true\")\n",
        "\n",
        "query = (\n",
        "    agg_df.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"parquet\")\n",
        "    .option(\"path\", \"/content/earthquakes/output\")\n",
        "    .option(\"checkpointLocation\", \"/content/earthquakes/checkpoint\")\n",
        "    .start()\n",
        ")\n"
      ],
      "metadata": {
        "id": "99wpSO7ioyLn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_and_save_earthquakes(stream_dir, iterations=3, sleep_seconds=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMg-53dHpDav",
        "outputId": "dbb432a5-5173-48f7-b899-fd99ed6305c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2192946674.py:8: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  print(f\"[{datetime.utcnow().isoformat()}] Llamando a la API ({i+1}/{iterations})...\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-11-26T00:58:22.615023] Llamando a la API (1/3)...\n",
            "  → 7 eventos recibidos\n",
            "  → Archivo escrito: /content/earthquakes/stream_data/earthquakes_1764118702.ndjson\n",
            "\n",
            "[2025-11-26T00:58:42.796682] Llamando a la API (2/3)...\n",
            "  → 7 eventos recibidos\n",
            "  → Archivo escrito: /content/earthquakes/stream_data/earthquakes_1764118722.ndjson\n",
            "\n",
            "[2025-11-26T00:59:02.934239] Llamando a la API (3/3)...\n",
            "  → 7 eventos recibidos\n",
            "  → Archivo escrito: /content/earthquakes/stream_data/earthquakes_1764118743.ndjson\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()\n"
      ],
      "metadata": {
        "id": "6D8Zp4fLpL6A"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = spark.read.parquet(\"/content/earthquakes/output\")\n",
        "df_results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "oUsb1uwbq1U-",
        "outputId": "e995a747-be18-4d81-a3ce-c94e279c0496"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Unable to infer schema for Parquet at . It must be specified manually.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2599363623.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/earthquakes/output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for Parquet at . It must be specified manually."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9djbhG5zq13G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}