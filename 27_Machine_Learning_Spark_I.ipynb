{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertoarturomc/ProgramacionConcurrente/blob/main/27_Machine_Learning_Spark_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UQAJS17EEaI"
      },
      "source": [
        "### Programación Concurrente\n",
        "## 27. Machine Learning con Spark I\n",
        "\n",
        "Para ver cómo era la versión \"Viejita\" (con `spark.mllib`), revisa este blog:\n",
        "\n",
        "https://medium.com/data-science-school/practical-apache-spark-in-10-minutes-part-4-mllib-fca02fecf5b8\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "XA6E9kNe6vrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "C7A4uqCv73BP",
        "outputId": "b6790652-3cf7-4dc1-b7d3-b9b6b55a5bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bc7ae7bcb60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://200bb9711175:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_iris()\n",
        "cols = [i.replace('(cm)','').strip().replace(' ','_') for i in data.feature_names] + ['species'] # Limpieza de los nombres de las columnas\n",
        "pdf = pd.DataFrame(np.c_[data.data, data.target], columns=cols)\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvl-0mEr7x2l",
        "outputId": "2c219c8a-afec-4202-8964-37993e2ef20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2|    0.0|\n",
            "|         4.9|        3.0|         1.4|        0.2|    0.0|\n",
            "|         4.7|        3.2|         1.3|        0.2|    0.0|\n",
            "|         4.6|        3.1|         1.5|        0.2|    0.0|\n",
            "|         5.0|        3.6|         1.4|        0.2|    0.0|\n",
            "|         5.4|        3.9|         1.7|        0.4|    0.0|\n",
            "|         4.6|        3.4|         1.4|        0.3|    0.0|\n",
            "|         5.0|        3.4|         1.5|        0.2|    0.0|\n",
            "|         4.4|        2.9|         1.4|        0.2|    0.0|\n",
            "|         4.9|        3.1|         1.5|        0.1|    0.0|\n",
            "|         5.4|        3.7|         1.5|        0.2|    0.0|\n",
            "|         4.8|        3.4|         1.6|        0.2|    0.0|\n",
            "|         4.8|        3.0|         1.4|        0.1|    0.0|\n",
            "|         4.3|        3.0|         1.1|        0.1|    0.0|\n",
            "|         5.8|        4.0|         1.2|        0.2|    0.0|\n",
            "|         5.7|        4.4|         1.5|        0.4|    0.0|\n",
            "|         5.4|        3.9|         1.3|        0.4|    0.0|\n",
            "|         5.1|        3.5|         1.4|        0.3|    0.0|\n",
            "|         5.7|        3.8|         1.7|        0.3|    0.0|\n",
            "|         5.1|        3.8|         1.5|        0.3|    0.0|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparamos nuestras variables objetivo y de entrenamiento.\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
        "    outputCol=\"features\"\n",
        ")"
      ],
      "metadata": {
        "id": "_Gbun-m6Wjbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializamos el Modelo\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100)"
      ],
      "metadata": {
        "id": "7Vvf_zD67yVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Iniciamos nuestro pipeline y separamos en un set de entrenamiento y otro de pruebas.\n",
        "pipeline = Pipeline(stages=[label_indexer, assembler, lr])\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "model = pipeline.fit(train)\n"
      ],
      "metadata": {
        "id": "26C0NtUi8FsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Notas cómo lo metimos dentro de un Pipeline? Esto nos ahorra tiempo, ya que de lo contrario tendríamos que estar dándole a un `.fit()` para cada parte del proceso."
      ],
      "metadata": {
        "id": "ZQDw_EzZewWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Evaluamos el modelo\n",
        "pred = model.transform(test)\n",
        "acc = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ").evaluate(pred)\n",
        "\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "\n",
        "pred.select(\"features\", \"prediction\", \"probability\").show(5, truncate=False)"
      ],
      "metadata": {
        "id": "NmuULdql8QTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d613cfcb-8526-4873-ce4a-0e3f998caafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 1.000\n",
            "+-----------------+----------+---------------------------------------------------+\n",
            "|features         |prediction|probability                                        |\n",
            "+-----------------+----------+---------------------------------------------------+\n",
            "|[4.4,3.0,1.3,0.2]|1.0       |[3.2846336132346933E-31,1.0,1.1713089090333092E-55]|\n",
            "|[4.6,3.2,1.4,0.2]|1.0       |[7.393509334952613E-34,1.0,1.5479602504474857E-58] |\n",
            "|[4.6,3.6,1.0,0.2]|1.0       |[2.5273919968001947E-45,1.0,1.3168692004627705E-72]|\n",
            "|[4.8,3.1,1.6,0.2]|1.0       |[3.8338070097546536E-29,1.0,6.765687668972633E-53] |\n",
            "|[4.9,3.1,1.5,0.2]|1.0       |[3.8004464378364086E-29,1.0,2.327617171474784E-53] |\n",
            "+-----------------+----------+---------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Y por qué no usar mejor Scikit-learn?\n",
        "\n",
        "La principal diferencia es que Scikit-learn **no** es compatible con Dataframes de Spark. Es necesario regresar a un Dataframe de Pandas, o un arreglo de Numpy, y esas transformaciones de formatos, como recordarás, son muy pesadas.\n",
        "\n",
        "Otra **gran** diferencia es que, siendo Scikit-learn más antiguo, tiene ya programada una mayor variedad de algoritmos, con mejor optimización de los mismos y más posibilidades de ajustar hiperparámetros.\n",
        "\n",
        "Por último, aunque algunos métodos y funciones son similares, la mayoría cambia en su sintáxis."
      ],
      "metadata": {
        "id": "LlT0bt_hkEGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tarea\n",
        "\n",
        "Investiga algunas formas en las que podemos hacer Feature Engineering usando `spark.ml`. Por ejemplo, ¿cómo creo variables dummy a través de variables categóricas? ¿Cómo estandarizo mis datos?"
      ],
      "metadata": {
        "id": "CaIdH2bSjL2w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3SM0Qy9e7-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}